{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Testing for a ML Model\n",
    "- We know that the input speeches can be arbitrarily long, so we first tried using premade transformers and adjusting settings in the,.\n",
    "- We first tried increasing the length of tokenizers and transformers\n",
    "- This is unsustainable, as the attention mechanisms scale non-linearly with input size\n",
    "- What we learned: another methodology is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\transformers\\configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "\n",
    "tokenizer = LEDTokenizer.from_pretrained(\"hyesunyun/update-summarization-bart-large-longformer\")\n",
    "model = LEDForConditionalGeneration.from_pretrained(\"hyesunyun/update-summarization-bart-large-longformer\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input read. Length is 4700\n"
     ]
    }
   ],
   "source": [
    "inputString = \"\"\n",
    "filePath = \"../testData/test.txt\"\n",
    "with open(filePath, \"r\") as file:\n",
    "    inputString = file.read()\n",
    "\n",
    "speakers = [\"Sean Casten\"]\n",
    "speakerText = \"\"\n",
    "if len(speakers) >  1:\n",
    "    speakerText = \"speeches by \" + [s for s in speakers].join(\", \")\n",
    "else:\n",
    "    speakerText = \"a speech by \" + speakers[0]\n",
    "prompt = f\"Please summarize this U.S. Government transcript of {speakerText}. Feel free to ignore things that look like this: '[[page number]]'\"\n",
    "inputString = prompt + inputString\n",
    "\n",
    "print(f\"Input read. Length is {len(inputString)}\")\n",
    "\n",
    "\n",
    "input = inputString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input = \"<EV> <t> Hypoglycemic effect of bitter melon compared with metformin in newly diagnosed type 2 diabetes patients. <abs> ETHNOPHARMACOLOGICAL RELEVANCE: Bitter melon (Momordica charantia L.) has been widely used as an traditional medicine treatment for diabetic patients in Asia. In vitro and animal studies suggested its hypoglycemic activity, but limited human studies are available to support its use. AIM OF STUDY: This study was conducted to assess the efficacy and safety of three doses of bitter melon compared with metformin. MATERIALS AND METHODS: This is a 4-week, multicenter, randomized, double-blind, active-control trial. Patients were randomized into 4 groups to receive bitter melon 500 mg/day, 1,000 mg/day, and 2,000 mg/day or metformin 1,000 mg/day. All patients were followed for 4 weeks. RESULTS: There was a significant decline in fructosamine at week 4 of the metformin group (-16.8; 95% CI, -31.2, -2.4 mumol/L) and the bitter melon 2,000 mg/day group (-10.2; 95% CI, -19.1, -1.3 mumol/L). Bitter melon 500 and 1,000 mg/day did not significantly decrease fructosamine levels (-3.5; 95% CI -11.7, 4.6 and -10.3; 95% CI -22.7, 2.2 mumol/L, respectively). CONCLUSIONS: Bitter melon had a modest hypoglycemic effect and significantly reduced fructosamine levels from baseline among patients with type 2 diabetes who received 2,000 mg/day. However, the hypoglycemic effect of bitter melon was less than metformin 1,000 mg/day. <EV> <t> Momordica charantia for type 2 diabetes mellitus. <abs> There is insufficient evidence to recommend momordica charantia for type 2 diabetes mellitus. Further studies are therefore required to address the issues of standardization and the quality control of preparations. For medical nutritional therapy, further observational trials evaluating the effects of momordica charantia are needed before RCTs are established to guide any recommendations in clinical practice.\"\n",
    "# 12 mins for this\n",
    "\n",
    "# tokenize the input\n",
    "inputs_dict = tokenizer(input, padding=\"max_length\", max_length=10240, return_tensors=\"pt\", truncation=True)\n",
    "input_ids = inputs_dict.input_ids\n",
    "attention_mask = inputs_dict.attention_mask\n",
    "global_attention_mask = torch.zeros_like(attention_mask)\n",
    "# put global attention on <s> token\n",
    "global_attention_mask[:, 0] = 1\n",
    "\n",
    "\n",
    "\n",
    "# Put our tensors on correct device\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "global_attention_mask = global_attention_mask.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Adam\\Desktop\\Projects\\flashGovProject\\flaskserver\\models\\longformer.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/longformer.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predicted_summary_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, global_attention_mask\u001b[39m=\u001b[39;49mglobal_attention_mask)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/longformer.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mbatch_decode(predicted_summary_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\transformers\\generation\\utils.py:1675\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1668\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1669\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1670\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   1671\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1672\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1673\u001b[0m     )\n\u001b[0;32m   1674\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1675\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[0;32m   1676\u001b[0m         input_ids,\n\u001b[0;32m   1677\u001b[0m         beam_scorer,\n\u001b[0;32m   1678\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[0;32m   1679\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[0;32m   1680\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[0;32m   1681\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[0;32m   1682\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[0;32m   1683\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[0;32m   1684\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[0;32m   1685\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[0;32m   1686\u001b[0m     )\n\u001b[0;32m   1688\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE:\n\u001b[0;32m   1689\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1690\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\transformers\\generation\\utils.py:3030\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3025\u001b[0m next_token_logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[0;32m   3026\u001b[0m next_token_scores \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(\n\u001b[0;32m   3027\u001b[0m     next_token_logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   3028\u001b[0m )  \u001b[39m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[1;32m-> 3030\u001b[0m next_token_scores_processed \u001b[39m=\u001b[39m logits_processor(input_ids, next_token_scores)\n\u001b[0;32m   3031\u001b[0m next_token_scores \u001b[39m=\u001b[39m next_token_scores_processed \u001b[39m+\u001b[39m beam_scores[:, \u001b[39mNone\u001b[39;00m]\u001b[39m.\u001b[39mexpand_as(next_token_scores)\n\u001b[0;32m   3033\u001b[0m \u001b[39m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\transformers\\generation\\logits_process.py:97\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     96\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores)\n\u001b[0;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\transformers\\generation\\logits_process.py:765\u001b[0m, in \u001b[0;36mNoRepeatNGramLogitsProcessor.__call__\u001b[1;34m(self, input_ids, scores)\u001b[0m\n\u001b[0;32m    763\u001b[0m num_batch_hypotheses \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    764\u001b[0m cur_len \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 765\u001b[0m banned_batch_tokens \u001b[39m=\u001b[39m _calc_banned_ngram_tokens(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mngram_size, input_ids, num_batch_hypotheses, cur_len)\n\u001b[0;32m    766\u001b[0m \u001b[39mfor\u001b[39;00m i, banned_tokens \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(banned_batch_tokens):\n\u001b[0;32m    767\u001b[0m     scores[i, banned_tokens] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\transformers\\generation\\logits_process.py:707\u001b[0m, in \u001b[0;36m_calc_banned_ngram_tokens\u001b[1;34m(ngram_size, prev_input_ids, num_hypos, cur_len)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mif\u001b[39;00m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m ngram_size:\n\u001b[0;32m    705\u001b[0m     \u001b[39m# return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[39mreturn\u001b[39;00m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)]\n\u001b[1;32m--> 707\u001b[0m generated_ngrams \u001b[39m=\u001b[39m _get_ngrams(ngram_size, prev_input_ids, num_hypos)\n\u001b[0;32m    708\u001b[0m banned_tokens \u001b[39m=\u001b[39m [\n\u001b[0;32m    709\u001b[0m     _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n\u001b[0;32m    710\u001b[0m     \u001b[39mfor\u001b[39;00m hypo_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)\n\u001b[0;32m    711\u001b[0m ]\n\u001b[0;32m    712\u001b[0m \u001b[39mreturn\u001b[39;00m banned_tokens\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\transformers\\generation\\logits_process.py:668\u001b[0m, in \u001b[0;36m_get_ngrams\u001b[1;34m(ngram_size, prev_input_ids, num_hypos)\u001b[0m\n\u001b[0;32m    666\u001b[0m generated_ngrams \u001b[39m=\u001b[39m [{} \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)]\n\u001b[0;32m    667\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos):\n\u001b[1;32m--> 668\u001b[0m     gen_tokens \u001b[39m=\u001b[39m prev_input_ids[idx]\u001b[39m.\u001b[39;49mtolist()\n\u001b[0;32m    669\u001b[0m     generated_ngram \u001b[39m=\u001b[39m generated_ngrams[idx]\n\u001b[0;32m    670\u001b[0m     \u001b[39m# Loop through each n-gram of size ngram_size in the list of tokens (gen_tokens)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "predicted_summary_ids = model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n",
    "print(tokenizer.batch_decode(predicted_summary_ids, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FlashGov_ML_Server",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
