{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This time it will work\n",
    "\n",
    "## Imports + Device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk # we need nltk to tokenize large inputs\n",
    "nltk.download('punkt') # punkt seems nessecary for it\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, pipeline, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Good to empty the cache out to ensure its clean on each new run\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Sanity checking\n",
    "print(\"torch cuda version:\", torch.version.cuda)\n",
    "\n",
    "# Check for cuda availability\n",
    "if(torch.cuda.is_available()):\n",
    "    deviceCount = torch.cuda.device_count()\n",
    "    currentNumber = torch.cuda.current_device()\n",
    "    deviceName = torch.cuda.get_device_name(currentNumber) \n",
    "    print(f\"Cuda available. {deviceCount} device(s) detected.\")\n",
    "    print(f\"Current Device: Number:{currentNumber} Name:{deviceName}\")\n",
    "else:\n",
    "    print(\"Cuda not available\")\n",
    "\n",
    "# Set device variable. All tensors and models must be on the SAME device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Model + Tokenizer\n",
    "- We select our model type (this will download and cache a huggingface model and transformer)\n",
    "- We examine the maximum inputs allowed for this model. This is very important to dictate our chunking method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model type we're using from Huggingface\n",
    "modelID = \"google/flan-t5-base\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(modelID).to(device)\n",
    "\n",
    "# Use the corresponding tokenizer for this model\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelID)\n",
    "\n",
    "# Sanity checking, show limits of the model\n",
    "\n",
    "maxInput = tokenizer.model_max_length # max length of input\n",
    "maxSentence = tokenizer.max_len_single_sentence # max len of a single sentince\n",
    "specialTokens = tokenizer.num_special_tokens_to_add() # tokenizer will add 2 special tokens for input seq\n",
    "\n",
    "print(f\"Max input length: {maxInput}, Max Sentence length: {maxSentence}, SpecialTokens: {specialTokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking and other Helpers\n",
    "- We need a way to split the input into digestable chunks for the transformer to operate on. This is chunking\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chunking function will iterate over sentinces of text and throw however many it can into a chunk.\n",
    "# This seems to be a quick way to break up the text in coherent places\n",
    "def chunkingFunction(fullTextString):\n",
    "    \n",
    "    # tokenize the large amount of text into sentinces\n",
    "    sentencesList = nltk.tokenize.sent_tokenize(fullTextString)\n",
    "\n",
    "    # check max length (in tokens) of all sentences\n",
    "    # if we exceed this anywhere we need to arbitrarily break it up\n",
    "    maxSentenceLen = max([len(tokenizer.tokenize(sentence)) for sentence in sentencesList])\n",
    "    \n",
    "    # This can be handled later, but for now we will raise an exception\n",
    "    if maxSentenceLen > maxSentence:\n",
    "        raise Exception(f\"Sentince length: {maxSentenceLen} exceeds the model's max sentince length: {maxSentence}\")\n",
    "    \n",
    "    # The current working chunk\n",
    "    workingChunk = \"\"\n",
    "    \n",
    "    # All completed chunks\n",
    "    completedChunks = []\n",
    "\n",
    "    length = 0\n",
    "    count = -1\n",
    "\n",
    "    \n",
    "    # Iter over all sentences\n",
    "    for sentence in sentencesList:\n",
    "        count += 1\n",
    "        combinedLength = len(tokenizer.tokenize(sentence)) + length\n",
    "        \n",
    "        # If the combined length is within permissable length\n",
    "        if(combinedLength < tokenizer.max_len_single_sentence):\n",
    "            \n",
    "            # Then add the sentence\n",
    "            workingChunk += sentence + \" \"\n",
    "            length = combinedLength\n",
    "            \n",
    "            # Also if this is the last chunk: strip whitespace and save it to completedChunks\n",
    "            if count == len(sentencesList) -1:\n",
    "                completedChunks.append(workingChunk.strip())\n",
    "        \n",
    "        # Otherwise, if adding this sentence breaches the maxmimum allowed chunks\n",
    "        else:\n",
    "            # Save the chunk we have, we can't add any more to it.\n",
    "            completedChunks.append(workingChunk.strip())\n",
    "            \n",
    "   \n",
    "            # RESET the working chunk\n",
    "            # Initialize it with the overflowing chunk and start again\n",
    "            workingChunk = sentence\n",
    "            length = len(tokenizer.tokenize(sentence))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FlashGov_ML_Server",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
