{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch cuda version: 12.1\n",
      "Cuda available. 1 device(s) detected.\n",
      "Current Device: Number:0 Name:NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#import all and setup cuda\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(\"torch cuda version:\", torch.version.cuda)\n",
    "# Check for cuda availability\n",
    "if(torch.cuda.is_available()):\n",
    "    deviceCount = torch.cuda.device_count()\n",
    "    currentNumber = torch.cuda.current_device()\n",
    "    deviceName = torch.cuda.get_device_name(currentNumber) \n",
    "    print(f\"Cuda available. {deviceCount} device(s) detected.\")\n",
    "    print(f\"Current Device: Number:{currentNumber} Name:{deviceName}\")\n",
    "else:\n",
    "    print(\"Cuda not available\")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input read. Length is 4558\n"
     ]
    }
   ],
   "source": [
    "# Get input\n",
    "\n",
    "inputString = \"\"\n",
    "filePath = \"../testData/test.txt\"\n",
    "with open(filePath, \"r\") as file:\n",
    "    inputString = file.read()\n",
    "\n",
    "print(f\"Input read. Length is {len(inputString)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading config.json: 100%|██████████| 853/853 [00:00<00:00, 853kB/s]\n",
      "c:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Adam\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading model.safetensors: 100%|██████████| 3.13G/3.13G [01:32<00:00, 33.8MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 2.34k/2.34k [00:00<?, ?B/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 7.94MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "#import the model\n",
    "from transformers import AutoModelForSeq2SeqLM, pipeline, AutoTokenizer\n",
    "\n",
    "\n",
    "modelID = \"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\"\n",
    "\n",
    "# Instantiate a pretrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(modelID)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Instantiate a pretrained tokenizer, use same as model\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max input length: 1000000000000000019884624838656, Max Sentence length: 1000000000000000019884624838655, SpecialTokens: 1\n"
     ]
    }
   ],
   "source": [
    "# Checking the limits we're working with\n",
    "maxInput = tokenizer.model_max_length # max length of input\n",
    "maxSentence = tokenizer.max_len_single_sentence # max len of a single sentince\n",
    "specialTokens = tokenizer.num_special_tokens_to_add() # tokenizer will add 2 special tokens for input seq\n",
    "print(f\"Max input length: {maxInput}, Max Sentence length: {maxSentence}, SpecialTokens: {specialTokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences:50, Max Sentence Len:63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Adam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# we need nltk to tokenize large inputs\n",
    "import nltk\n",
    "# punkt seems nessecary\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = nltk.tokenize.sent_tokenize(inputString)\n",
    "sentenceLen = len(sentences)\n",
    "\n",
    "# check max token length of all sentences once tokenized\n",
    "maxSentenceLen = max([len(tokenizer.tokenize(sentence)) for sentence in sentences])\n",
    "\n",
    "print(f\"Num of sentences:{sentenceLen}, Max Sentence Len:{maxSentenceLen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1038]\n",
      "[1039]\n",
      "1038\n",
      "1038\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# holds working chunk seperated by whitespace\n",
    "chunk = \"\"\n",
    "# holds all completed chunks\n",
    "chunks = []\n",
    "\n",
    "length = 0\n",
    "count = -1\n",
    "\n",
    "for sentence in sentences:\n",
    "    count += 1\n",
    "    combined_length = len(tokenizer.tokenize(sentence)) + length\n",
    "    \n",
    "    # If the combined length is within permissable length\n",
    "    if(combined_length < tokenizer.max_len_single_sentence):\n",
    "        \n",
    "        # and the sentince and length to our working chunk\n",
    "        chunk += sentence + \" \"\n",
    "        length = combined_length\n",
    "        \n",
    "        # if this is the last chunk, strip whitespace and save it in chunks\n",
    "        if count == len(sentences) -1:\n",
    "            chunks.append(chunk.strip())\n",
    "    \n",
    "    # If it breaches the maxmimum allowed chunks\n",
    "    else:\n",
    "        # save the chunk we have\n",
    "        chunks.append(chunk.strip())\n",
    "        \n",
    "        # reset the length and chunk\n",
    "        length = 0\n",
    "        chunk = \"\"\n",
    "        \n",
    "        # add the overflowing chunk and update length\n",
    "        chunk += sentence + \" \"\n",
    "        length = len(tokenizer.tokenize(sentence))\n",
    "        \n",
    "        \n",
    "# =============== Sanity Checks =============== #\n",
    "# How many chunks we have\n",
    "print(len(chunks))\n",
    "# How many tokens are in each chunk (excluding special tokens)\n",
    "print([len(tokenizer.tokenize(c)) for c in chunks])\n",
    "# Number of tokens in eeach chunk (including spacial tokens)\n",
    "print([len(tokenizer(c).input_ids) for c in chunks])\n",
    "# total number of tokens in all chunks\n",
    "print(sum([len(tokenizer.tokenize(c)) for c in chunks]))\n",
    "# this should be close to the total number of chunks\n",
    "# if it's not, thats because we removed extra whitespaces while stripping\n",
    "print(len(tokenizer.tokenize(inputString)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our inputs using the tokenizer on our chunks\n",
    "# Send them to the correct device while we're at it\n",
    "inputs = [tokenizer(chunk, return_tensors=\"pt\").to(device) for chunk in chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "The SPEAKER pro tempore. The Chair recognizes the gentleman from \n",
      "Illinois (Mr. Casten) for 5 minutes.\n",
      "  Mr. CASTEN. Mr. Speaker, I want you to imagine that you get home from \n",
      "work tomorrow and decide that you just don't want to pay your mortgage, \n",
      "so you call your bank and share the good news.\n",
      "  The bank, at that point, explains that if you don't, you are looking \n",
      "at eviction, so you propose a counteroffer because you are a clever \n",
      "fellow. I will pay my mortgage, but only if my family stops using hot \n",
      "water and stops going to the doctor. At which point, the bank says: \n",
      "You, sir, are a moron.\n",
      "  Yet, that is what is happening in the House this week. The United \n",
      "States Government has already agreed to buy a house. We are already \n",
      "living in the house. We took out a loan to pay for the house, but the \n",
      "Republicans want to stop making the mortgage payments unless the \n",
      "American people agree to skimp on healthcare and basic services.\n",
      "  I would remind everybody in this body that it is Congress that has \n",
      "the power of the purse. Every dollar of government spending and every \n",
      "dollar of government revenue was approved on this floor by this body. \n",
      "If the expenses exceed the revenues, then we borrow to make up the \n",
      "difference. We made those choices.\n",
      "  You all may not like every dollar that we have approved. I certainly \n",
      "don't. I also don't agree with every dollar my wife has ever spent, and \n",
      "she doesn't agree with every dollar that I have ever spent, but we get \n",
      "by because my wife and I are functioning adults. That would appear to \n",
      "be too high a bar for this body.\n",
      "  The majority today is bringing a package to the floor that, according \n",
      "to Moody's Analytics, would ``meaningfully increase the likelihood'' of \n",
      "a recession; not because it is the right thing to do but because it is \n",
      "the only way that the Speaker can keep his job.\n",
      "  If Speaker McCarthy brought a clean debt limit bill to the floor of \n",
      "the House tomorrow, he would get every single Democratic vote, and I am \n",
      "sure a few of my colleagues across the aisle would join in. It would \n",
      "pass the Senate, and President Biden would sign it. That would be that. \n",
      "We would move on to the important work of this country.\n",
      "  But as all of us know who spent a long, late night here after 14 \n",
      "failed efforts to win the Speakership, Mr. McCarthy caved to the \n",
      "extremists in his own party and agreed to conditions where he loses his \n",
      "job if he does not try to blow up the economy. He has the opportunity \n",
      "today to do the right thing; he simply doesn't have the ability.\n",
      "  The House is now preparing to vote on a bill that would be disastrous \n",
      "for Americans. Let's be clear: If we don't raise the debt ceiling, we \n",
      "will see collapses in global financial markets.\n",
      "  We will default on our debt, which means interest rates go up, which \n",
      "means everything you have that has an interest rate tied to it gets \n",
      "more expensive. Your mortgage, your car payment, your student loan, \n",
      "your credit card payments, all those things start going up.\n",
      "  The majority is hoping that the adults in the room will prevail and \n",
      "that the adults will be so frightened by that consequence that we will \n",
      "accept their self-destructive alternative.\n",
      "  The alternative is a bill that would slash nondefense discretionary \n",
      "spending by 22 percent. Just in my home State of Illinois, let's talk \n",
      "about what that means.\n",
      "  That means in Illinois, home of the O'Hare airport hub, eight traffic \n",
      "control centers shut down.\n",
      "  In the wake of East Palestine, the train crash, just in Illinois, we \n",
      "will do 420 fewer rail inspections.\n",
      "  2,900 students in Illinois will lose their Pell grants. Several \n",
      "hundred thousand will see their Pell grants cut.\n",
      "  10,000 kids will lose pre-K and childcare.\n",
      "  53,000 veterans will lose outpatient services. That is just in \n",
      "Illinois.\n",
      "  Nationally, a cleaner, cheaper future will be thrown away.\n",
      "  White-collar police will be defunded to make it easier for everybody \n",
      "to commit tax fraud, which, of course, will lower future revenues and \n",
      "make it necessary for us to borrow more money.\n",
      "  The majority would have us believe that our only choice right now is: \n",
      "Do we want to blow up the economy, or do\n",
      "\n",
      "[[Page H1954]]\n",
      "\n",
      "we want to ruin the lives of millions of Americans? That is not the \n",
      "only choice.\n",
      "  We have a third option. The third option is that we can let the \n",
      "adults come back in the room. We can take our jobs seriously. We can \n",
      "pass a clean debt limit bill.\n",
      "  Governing is hard, but it is especially hard when you let the clowns \n",
      "drive the car.\n",
      "  Please, Mr. Speaker, let the adults take the wheel.\n",
      "\n",
      "output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\transformers\\modeling_utils.py:854: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the majority of the government is hoping that the adults in the room will prevail and that the adults will be so frightened by that consequence that we will accept their self-destructive alternative. the alternative is a bill that would slash nondefense discretionary spending by 22 percent. the majority would have us believe that our only choice right now is: do we want to blow up the economy or do [[page ] we want to ruin the lives of millions of people?\n"
     ]
    }
   ],
   "source": [
    "# F\n",
    "print(\"input:\")\n",
    "print(inputString)\n",
    "print(\"output:\")\n",
    "for input in inputs:\n",
    "    output = model.generate(**input, max_length=200)\n",
    "    print(tokenizer.decode(*output, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
