{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch cuda version: 12.1\n",
      "Cuda available. 1 device(s) detected.\n",
      "Current Device: Number:0 Name:NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#import all and setup cuda\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(\"torch cuda version:\", torch.version.cuda)\n",
    "# Check for cuda availability\n",
    "if(torch.cuda.is_available()):\n",
    "    deviceCount = torch.cuda.device_count()\n",
    "    currentNumber = torch.cuda.current_device()\n",
    "    deviceName = torch.cuda.get_device_name(currentNumber) \n",
    "    print(f\"Cuda available. {deviceCount} device(s) detected.\")\n",
    "    print(f\"Current Device: Number:{currentNumber} Name:{deviceName}\")\n",
    "else:\n",
    "    print(\"Cuda not available\")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input read. Length is 4700\n"
     ]
    }
   ],
   "source": [
    "# Get input\n",
    "\n",
    "inputString = \"\"\n",
    "filePath = \"../testData/test.txt\"\n",
    "with open(filePath, \"r\") as file:\n",
    "    inputString = file.read()\n",
    "\n",
    "speakers = [\"Sean Casten\"]\n",
    "speakerText = \"\"\n",
    "if len(speakers) >  1:\n",
    "    speakerText = \"speeches by \" + [s for s in speakers].join(\", \")\n",
    "else:\n",
    "    speakerText = \"a speech by \" + speakers[0]\n",
    "prompt = f\"Please summarize this U.S. Government transcript of {speakerText}. Feel free to ignore things that look like this: '[[page number]]'\"\n",
    "inputString = prompt + inputString\n",
    "\n",
    "print(f\"Input read. Length is {len(inputString)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 990M/990M [00:42<00:00, 23.4MB/s] \n",
      "Downloading generation_config.json: 100%|██████████| 147/147 [00:00<?, ?B/s] \n",
      "Downloading tokenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<?, ?B/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 60.9MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 8.19MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "#import the model\n",
    "from transformers import AutoModelForSeq2SeqLM, pipeline, AutoTokenizer\n",
    "\n",
    "\n",
    "modelID = \"google/flan-t5-base\"\n",
    "\n",
    "# Instantiate a pretrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(modelID)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Instantiate a pretrained tokenizer, use same as model\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max input length: 512, Max Sentence length: 511, SpecialTokens: 1\n"
     ]
    }
   ],
   "source": [
    "# Checking the limits we're working with\n",
    "maxInput = tokenizer.model_max_length # max length of input\n",
    "maxSentence = tokenizer.max_len_single_sentence # max len of a single sentince\n",
    "specialTokens = tokenizer.num_special_tokens_to_add() # tokenizer will add 2 special tokens for input seq\n",
    "print(f\"Max input length: {maxInput}, Max Sentence length: {maxSentence}, SpecialTokens: {specialTokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences:51, Max Sentence Len:63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Adam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# we need nltk to tokenize large inputs\n",
    "import nltk\n",
    "# punkt seems nessecary\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = nltk.tokenize.sent_tokenize(inputString)\n",
    "sentenceLen = len(sentences)\n",
    "\n",
    "# check max token length of all sentences once tokenized\n",
    "maxSentenceLen = max([len(tokenizer.tokenize(sentence)) for sentence in sentences])\n",
    "\n",
    "print(f\"Num of sentences:{sentenceLen}, Max Sentence Len:{maxSentenceLen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[507, 501, 67]\n",
      "[508, 502, 68]\n",
      "1075\n",
      "1076\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# holds working chunk seperated by whitespace\n",
    "chunk = \"\"\n",
    "# holds all completed chunks\n",
    "chunks = []\n",
    "\n",
    "length = 0\n",
    "count = -1\n",
    "\n",
    "for sentence in sentences:\n",
    "    count += 1\n",
    "    combined_length = len(tokenizer.tokenize(sentence)) + length\n",
    "    \n",
    "    # If the combined length is within permissable length\n",
    "    if(combined_length < tokenizer.max_len_single_sentence):\n",
    "        \n",
    "        # and the sentince and length to our working chunk\n",
    "        chunk += sentence + \" \"\n",
    "        length = combined_length\n",
    "        \n",
    "        # if this is the last chunk, strip whitespace and save it in chunks\n",
    "        if count == len(sentences) -1:\n",
    "            chunks.append(chunk.strip())\n",
    "    \n",
    "    # If it breaches the maxmimum allowed chunks\n",
    "    else:\n",
    "        # save the chunk we have\n",
    "        chunks.append(chunk.strip())\n",
    "        \n",
    "        # reset the length and chunk\n",
    "        length = 0\n",
    "        chunk = \"\"\n",
    "        \n",
    "        # add the overflowing chunk and update length\n",
    "        chunk += sentence + \" \"\n",
    "        length = len(tokenizer.tokenize(sentence))\n",
    "        \n",
    "        \n",
    "# =============== Sanity Checks =============== #\n",
    "# How many chunks we have\n",
    "print(len(chunks))\n",
    "# How many tokens are in each chunk (excluding special tokens)\n",
    "print([len(tokenizer.tokenize(c)) for c in chunks])\n",
    "# Number of tokens in eeach chunk (including spacial tokens)\n",
    "print([len(tokenizer(c).input_ids) for c in chunks])\n",
    "# total number of tokens in all chunks\n",
    "print(sum([len(tokenizer.tokenize(c)) for c in chunks]))\n",
    "# this should be close to the total number of chunks\n",
    "# if it's not, thats because we removed extra whitespaces while stripping\n",
    "print(len(tokenizer.tokenize(inputString)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our inputs using the tokenizer on our chunks\n",
    "# Send them to the correct device while we're at it\n",
    "inputs = [tokenizer(chunk, return_tensors=\"pt\").to(device) for chunk in chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      "The U.S. Government transcript of a speech by Sean Casten.\n",
      "The House would have voted to raise the debt ceiling, and President Biden would sign it.\n",
      "We can pass a clean debt limit bill.\n"
     ]
    }
   ],
   "source": [
    "# F\n",
    "# print(\"input:\")\n",
    "# print(inputString)\n",
    "print(\"output:\")\n",
    "for input in inputs:\n",
    "    output = model.generate(**input, max_length=120)\n",
    "    print(tokenizer.decode(*output, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
