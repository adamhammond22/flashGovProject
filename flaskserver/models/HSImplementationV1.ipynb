{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second attempt\n",
    "- Here we tried to create a cleaner setup with reusable code\n",
    "- We created a model and testing loop, with the idea of adding a chunking function into this setup\n",
    "- We want to use the strategy of \"Hierarchical summarization\" and implement this into the model's prediction function for training and testing.\n",
    "- The problem with this (and many tutorials) is that the generation of a hierarchical summarization can't be turned into just one function easily, and plugged into an existing setup.\n",
    "- We did however find a good dataset we'd like to use to train the model.\n",
    "- What we learned: Careful preperation and pre-planning is needed to implement a training loop using hierarchical summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initalization\n",
    "Install packages and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Adam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import all prereqs, set vars\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "nltk.download(\"punkt\")\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# init our model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Helpers\n",
    "Define our helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We need to split the data into batches so we can process them in chunks\n",
    "# We can't load the entire dataset into memory\n",
    "# this is a generator\n",
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "\n",
    "# This runs the model on the dataset in batches, and calculates the metric\n",
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer, \n",
    "                               batch_size=16, device=device, \n",
    "                               column_text=\"report\", \n",
    "                               column_summary=\"summary\"):\n",
    "\n",
    "    #Get the batch of text and associated summary\n",
    "    text_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "    print(\"finished text batches\")\n",
    "    # Iterate over each batch\n",
    "    for text_batch, target_batch in tqdm(zip(text_batches, target_batches), total=len(text_batches)):\n",
    "        print(\"batch run len:\", len(text_batch))\n",
    "        \n",
    "        # Tokenize the input batch\n",
    "        inputs = tokenizer(text_batch, max_length=1024,  truncation=True, \n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "        print(\"tokenized\")\n",
    "        # Generate summaries for the input batch\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device), \n",
    "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
    "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "        print(\"summarized\")\n",
    "        # Decode or de-tokenize summaries into real text so we can evaluate it\n",
    "        # replace the  token, and add the decoded texts with the references to the metric.\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n",
    "                                clean_up_tokenization_spaces=True) \n",
    "               for s in summaries]      \n",
    "        print(\"decoded\")\n",
    "        # Remove whitespace in summaries\n",
    "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "        \n",
    "        \n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "        print(\"added batch\")\n",
    "    #  Finally compute and return the ROUGE scores.\n",
    "    score = metric.compute()\n",
    "    return score\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: govreport-summarization/document\n",
      "Found cached dataset govreport-summarization (C:/Users/Adam/.cache/huggingface/datasets/ccdv___govreport-summarization/document/1.0.0/57ca3042de9c40c218cc94084cbc80a99a161036134bfc88112c57d251443590)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269edd2112694049bc1c59dca6457778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset:\n",
      "Set 'train': rows:17517, features:['report', 'summary']\n",
      "Set 'validation': rows:973, features:['report', 'summary']\n",
      "Set 'test': rows:973, features:['report', 'summary']\n",
      "\n",
      "Report:\n",
      "A variety of federal laws, regulations, and policies establish requirements and guidance for EPA to follow when appointing members to serve on advisory committees. For example, one purpose of FACA is to ensure that uniform procedures govern the establishment and operation of advisory committees. Also under FACA, an agency establishing an advisory committee must, among other things, require the committee’s membership to be balanced in terms of the points of view represented and the functions to be performed by the committee. In addition, federal ethics regulations establish when and how federal officials should review financial disclosure forms to identify and prevent conflicts of interest prohibited by federal law for any prospective committee members required to file these forms in connection with their appointments to advisory committees. GSA has provided additional guidance regarding the implementation of ethics requirements under FACA. Various EPA offices and officials are responsible for helping the agency follow these requirements. For example, EPA’s Federal Advisory Committee Management Division—which has overall responsibility for committee management and ensuring that EPA’s advisory committees comply with FACA—developed the Federal Advisory Committee Handbook to clarify roles and responsibilities for complying with relevant requirements. The handbook was written primarily for EPA employees assigned as designated federal officers for committees. These officers are responsible for the day-to-day management of advisory committees and play a central role in identifying and recommending candidates who can help the committees meet their goals. EPA employees assigned as designated federal officers also are responsible for maintaining committee records. According to EPA’s Federal Advisory Committee Handbook, one of the primary reasons that Congress passed FACA was to ensure public access to the records and documents of advisory committees, and that this fosters greater transparency and accountability of agencies’ use of advisory committees. EPA’s Ethics Office is responsible for helping the agency follow federal ethics requirements. Housed within the agency’s Office of General Counsel in headquarters, the Ethics Office oversees all aspects of the agency’s ethics program, including financial disclosure reporting. The Designated Agency Ethics Official coordinates and manages the program. The Designated Agency Ethics Official delegates authority to more than 100 deputy ethics officials located throughout the agency— including in headquarters and regional offices—to carry out most elements of EPA’s ethics program. For example, deputy ethics officials are to review financial disclosure reports for prospective committee members to identify and prevent conflicts of interest. Deputy assistant administrators, deputy regional administrators, office directors, and other EPA managers may be appointed to serve as deputy ethics officials for their offices as ancillary duties to their other responsibilities. EPA can establish two kinds of advisory committees—non-discretionary and discretionary committees. The agency establishes non- discretionary committees when required to by statute or directed to by the President. For example, the Clean Air Act requires EPA to establish an advisory committee to, among other things, help EPA review standards for national ambient air quality every 5 years. EPA also can establish discretionary committees at the Administrator’s direction if, for example, these committees provide an important and unique perspective on EPA programs or operations. An example of a discretionary committee is the Pesticide Program Dialogue Committee, which was formed to help EPA perform its duties under the Federal Insecticide, Fungicide and Rodenticide Act and related laws. See appendix II for a list of EPA’s 22 advisory committees as of March 31, 2018. EPA must approve the establishment of any subcommittees formed to assist committees with their work. EPA also can appoint different types of members to its advisory committees, depending on the needs of its committees and other considerations. For instance, EPA may appoint a committee member as a federal government employee under an appropriate hiring authority. If EPA expects a federal employee to serve no more than 130 days in any 365-day period, guidance from the U.S. Office of Government Ethics (OGE), which oversees the executive branch’s ethics program, states that the employee should be designated as a special government employee (SGE). If EPA decides not to appoint the committee member as a federal employee, that committee member would be a non-employee representative. EPA decides whether to appoint committee members as federal employees. To help federal agencies such as EPA determine whether to designate committee members as SGEs or representatives, OGE has developed guidance on factors to consider when agencies make these determinations. For example, OGE guidance states that SGEs are expected to provide independent expert advice and provide their best judgment free from conflicts of interest. They are generally subject to federal ethics regulations placed on other federal employees—including the requirement to file financial disclosure forms. In addition, OGE guidance states that representatives serve as the voice of groups or entities with a financial or other stake in a particular matter before an advisory committee. Federal ethics regulations generally do not apply to representative members on FACA committees. GSA has certain government-wide responsibilities for implementing FACA, including maintaining the government-wide FACA database that tracks certain characteristics of advisory committees. Specifically, FACA requires GSA to comprehensively review the activities and responsibilities of each advisory committee annually, including the committees for which EPA officials are responsible. In turn, GSA requires federal agencies responsible for advisory committees to enter data about those committees into the database. GSA and the responsible agency (e.g., EPA) review the data on a fiscal year basis for accuracy and completeness. These reviews are typically completed by February or March of the following year. GSA’s database is accessible by the general public. It includes data on committee members and committee activities from more than 50 agencies going back to 1997. The information on EPA committees includes: whether a committee member is designated as an SGE or representative; the occupation or affiliation of a committee member; state or other geographic information associated with a committee member’s occupation or affiliation; the appointment’s start and end date for each committee member; and the dates that committees held meetings. Based on our review of EPA’s Federal Advisory Committee Handbook, the agency’s established process for appointing advisory committee members includes three main phases. These phases are soliciting nominations, evaluating candidates, and obtaining approvals from relevant EPA offices, such as the Federal Advisory Committee Management Division, before the Administrator or Deputy Administrator makes final appointment decisions. As shown in figure 1, each of the three main phases in EPA’s process involves several smaller steps. Unless noted otherwise, explanations of these steps can be found in the handbook, which documents the agency’s established process. Soliciting nominations involves six basic steps, which are carried out by a committee’s designated federal officer. The steps are as follows: Develop selection criteria. This step involves identifying the specific perspectives or points of view that should be represented by members on the committee, such as specific scientific perspectives or understandings of environmental justice. This step applies to both discretionary and non-discretionary committees. In addition, federal laws establish membership requirements for the agency’s non- discretionary committees that designated federal officers must consider when developing selection criteria. For example, the Clean Air Act requires EPA to appoint seven members—including at least one member of the National Academy of Sciences, one physician, and one person representing state air-pollution control agencies—to an independent scientific advisory committee, known as CASAC. The selection criteria developed in this step should be reflected in the notice soliciting nominations. Develop an outreach plan. This plan should: (1) describe in detail how committees intend to solicit a diverse set of nominees and (2) discuss the specific forms of solicitation. For example, one outreach plan we reviewed specified that EPA staff would solicit nominations from the American Academy of Pediatrics, American Chemical Society, and other organizations that can help EPA review the quality, relevance, and performance of its research programs. Develop membership balance plans for discretionary committees. GSA guidance states that membership balance plans for discretionary committees should describe the process used to ensure that committee membership is balanced in terms of the points of view represented and functions to be performed by the committee. For example, one membership balance plan we reviewed stated that EPA staff would consider candidates from farm worker organizations; pesticide industry and trade associations; state, local and tribal governments; and public health and other organizations. According to that membership balance plan, EPA staff also would consider prospective committee members’ geographic location to help achieve balanced membership. Solicit nominations. During this step, the designated federal officer can solicit nominations via Federal Register notices and other means, such as emails to professional associations and specific EPA email distribution lists. In response to these notices, organizations can nominate individuals, or individuals can nominate themselves or other individuals. Contact nominees after receiving nominations. During this step, the designated federal officer confirms nominees’ qualifications and experience as well as their interest in and availability to serve on the committee. Assess the diversity of the pool of nominees and conduct additional outreach, if needed, to increase the diversity of the pool. EPA’s Federal Advisory Committee Handbook provides illustrative examples of how to follow this step. In one example, the handbook explains that a committee needs a representative from local government. For the past several years, the position has been filled by someone from an affluent suburban county. To increase diversity, the handbook recommends that the designated federal officer broaden outreach to other parts of the country, especially local governments that serve low-income, rural, urban, medically underserved, or vulnerable populations. Evaluating candidates similarly involves several steps. The committee’s designated federal officer is primarily responsible for taking these steps for his or her assigned committee. In addition, a deputy ethics official is to review financial disclosure forms for any prospective members who are required to file these forms. In general, the steps for evaluating candidates are as follows: Evaluate candidates against selection criteria. During this step, the designated federal officer identifies the specific point of view that each candidate would bring to the committee—as well as each candidate’s ability to meet the selection criteria after interviewing candidates and reviewing their curriculum vitae, publications, and other relevant information. EPA’s Federal Advisory Committee Handbook notes that having the best people who represent key interests and balanced viewpoints enables the committee to provide EPA with recommendations that the agency can rely on as collective advice representing diverse stakeholder views. Identifying the best candidates may involve reviewing many more nominees than can be appointed. For example, EPA received approximately 100 nominations for 18 positions on the Science Advisory Committee on Chemicals in fiscal year 2017. Prepare a draft membership grid document with staff- recommended candidates and alternates. After evaluating individual candidates, the handbook directs the designated federal officer to recommend at least one primary and alternate candidate for each point of view and consolidate his or her short-list of recommended candidates into a draft membership grid document. The handbook indicates that this is a key step in the agency’s appointment process. It is intended to help designated federal officers identify gaps as they seek to meet FACA requirements for balanced committee membership. The handbook also directs the designated federal officer to submit the draft membership grid to EPA’s Federal Advisory Committee Management Division, EPA’s Office of General Counsel, and the Assistant Administrator for review and approval before submitting final recommendations to the Administrator. Therefore, the draft membership grid, which documents EPA staff’s rationale for recommending specific candidates, is intended to serve as the basis for discussions with EPA management as final decisions about the committee’s composition are made, according to EPA’s Federal Advisory Committee Handbook. Recommending at least one alternate for each point of view is intended to provide the EPA Administrator or Deputy Administrator—who officially selects committee members based on staff recommendations—with flexibility in appointing members, according to the handbook. Review financial disclosure forms for conformance with applicable conflict-of-interest statutes, regulations issued by OGE including any supplemental agency requirements, and other federal ethics rules, which state, among other things, that: SGEs appointed to serve on federal advisory committees generally must file financial disclosure forms within 30 days of assuming their new positions and either before providing advice to the agency or before the first committee meeting if they are eligible to file confidentially. The designated ethics official from each executive branch agency generally is to review financial disclosure reports within 60 days after receiving them and is to certify by signature and date that the filer is in compliance with federal ethics rules, and this official generally may delegate this responsibility. Obtaining approvals involves several steps and numerous EPA officials. The steps for obtaining approvals generally are as follows: EPA’s Federal Advisory Committee Management Division reviews the proposed membership for balance. EPA guidance states that designated federal officers are to obtain written concurrence from the division before preparing the final membership package for the Administrator to sign. EPA’s Office of General Counsel conducts a legal review of the proposed membership. EPA guidance states that designated federal officers are to obtain written concurrence from the Office of General Counsel prior to appointment. Assistant Administrator or Regional Administrator approves the list of recommended candidates that will be presented to the Administrator’s office. Administrator or Deputy Administrator makes final appointment decisions and signs appointment letters. From fiscal year 2017 through the first two quarters of fiscal year 2018, EPA generally followed its established process for most advisory committees; however, in fiscal year 2018, EPA did not follow a key step in its process for appointing 20 committee members to the SAB and CASAC. SAB is the agency’s largest committee and CASAC is responsible for, among other things, reviewing national ambient air-quality standards. In addition, when reviewing the step in EPA’s appointment process related specifically to financial disclosure reporting, we found that EPA did not consistently ensure that SGEs appointed to advisory committees met federal financial disclosure requirements. Our review of agency documents that supported appointment decisions for the 17 committees that appointed or reappointed committee members from fiscal year 2017 through the first two quarters of fiscal year 2018 found that EPA generally followed its process for most committees. All 14 of the discretionary committees that appointed or reappointed members during this time period developed membership balance plans, as required by GSA’s FACA regulations. In addition, 15 committees followed the step in EPA’s appointment process related to draft membership grid documents. That is, 20 of the 22 appointment packets we reviewed had draft membership grid documents reflecting EPA staff input on the best qualified and most appropriate candidates for achieving balanced committee membership. Additionally, 21 of the 22 appointment packets we reviewed contained documentation showing that EPA’s Office of General Counsel reviewed the proposed membership prior to appointment, as recommended by EPA’s Federal Advisory Committee Handbook. Figure 2 shows EPA’s established process and the steps we reviewed. For additional information about the extent to which EPA followed its process for appointing committee members, see appendix III. However, EPA did not follow a key step in its established process for appointing 20 members in fiscal year 2018 to the SAB and CASAC, which advise the agency on environmental regulatory matters, among other things. Specifically, the fiscal year 2018 appointment packets for the SAB and CASAC did not include draft membership grid documents reflecting EPA staff rationales for recommending the candidates EPA’s staff deem best qualified and most appropriate for achieving balanced committee membership. EPA officials told us in March 2019 that they did not prepare draft membership grids, as recommended by EPA’s Federal Advisory Committee Handbook, because EPA management requested a series of briefings instead. EPA officials also told us that during these briefings, EPA staff presented options for management to consider that reflected staff evaluations and summaries of public comments on candidates. EPA management then decided whom to appoint after reviewing the entire list of personnel nominated for membership—not a short-list of staff-recommended candidates, as called for by EPA’s handbook. During previous appointment cycles, EPA documents indicate and officials told us that EPA followed its established process when appointing committee members to SAB and CASAC. Specifically, documents from SAB’s and CASAC’s fiscal year 2017 appointment cycles indicate that both committees prepared draft membership grids in fiscal year 2017 in accordance with EPA’s established process. In addition, SAB and CASAC staff we interviewed told us that the process they used for filling vacancies prior to the fiscal year 2018 appointments involved vetting candidates before documenting in draft membership grids the candidates they deemed best qualified and most appropriate for achieving balanced committees. EPA officials stated that the briefing process they used in fiscal year 2018 was considered better than the use of draft membership grids, as it allowed EPA management to have in-depth discussions with SAB staff, resulting in better knowledge and a greater understanding of the SAB’s and CASAC’s membership needs. In written comments on the draft report, EPA stated that the vetting of candidates for SAB and CASAC occurred in a different manner than in previous years with a process more robust than membership grids. In addition, EPA stated that the public comment process was more robust, going beyond what was prescribed in the traditional membership process. There may be benefits to such discussions and solicitation of input. However, under EPA’s established process, agency staff are to document in draft membership grids and include in appointment packets their rationales for recommending the candidates they deem best qualified and most appropriate for achieving balanced committees. EPA developed guidance to implement FACA, one purpose of which is to encourage the establishment of uniform committee appointment and administration procedures. In written comments on the draft report, EPA noted that agency staff documented evaluations of advisory committee candidates in briefing documents. However, EPA did not provide these documents along with its comments. Moreover, neither these evaluations nor summaries of public comments were included in the packets that EPA’s Federal Advisory Committee Handbook indicates are to contain committee appointment information, impeding EPA’s ability to ensure that it consistently meets—across all of its advisory committees—FACA’s purpose of encouraging uniform committee appointment procedures. In addition, Federal Standards for Internal Control call for management to design control activities to achieve objectives and respond to risks, such as by clearly documenting all transactions and other significant events in a manner that allows the documentation to be readily available for examination. By directing officials responsible for appointing committee members to follow a key step in EPA’s appointment process—developing draft membership grids to document staff rationales for proposed membership—the agency would also have better assurance that it could show how it made appointment decisions to achieve the best qualified and most appropriate candidates for balanced membership. When reviewing the steps in EPA’s appointment process related specifically to financial disclosure reporting, we found that from fiscal year 2017 through the first two quarters of fiscal year 2018, EPA did not consistently ensure that 74 SGEs appointed or reappointed to serve on EPA advisory committees met federal financial-disclosure requirements. Of the 74 disclosure forms we reviewed, an ethics official signed and dated that the filer was in compliance with federal ethics rules for 77 percent, or 57 of the forms. However, for about 23 percent, or 17 of the 74 financial disclosure forms we reviewed, an ethics official had not signed and dated that the filer was in compliance with federal ethics rules. In addition, for about 57 percent, or 42 of the 74 forms we reviewed, we were unable to determine whether an ethics official had reviewed the financial disclosure forms within 60 days after they were filed because the forms did not indicate when EPA had received them. Table 1 illustrates the extent to which EPA took steps to ensure compliance with federal financial-disclosure-reporting requirements relevant to SGEs during this time period. In 2017, OGE found similar weaknesses in EPA’s ethics program. For example, when OGE reviewed a sample of EPA advisory committees’ ethics documents from 2015, it found that none of the financial disclosure forms for one committee had been reviewed—or signed and dated—by an ethics official to indicate that filers were in conformance with federal ethics rules. For two other committees, OGE found that EPA had not received in 2015 certain financial-disclosure forms that were due that year. We also found that EPA’s Ethics Office had not periodically evaluated, through audits or spot-checks, the quality of financial disclosure reviews conducted by its deputy ethics officials for SGEs appointed to advisory committees, as part of the periodic review of its ethics program called for by OGE regulations. An official we interviewed from EPA’s Ethics Office told us that the office did not have the staffing levels necessary to audit or spot-check financial disclosure reviews for SGEs. In addition, in a June 2018 correspondence to OGE about OGE’s review of EPA’s ethics program, EPA’s Designated Agency Ethics Official stated that EPA’s Ethics Office had fewer than three full-time equivalent positions at times during 2017. The correspondence also stated that the agency’s Office of General Counsel is committed to doubling the Ethics Office’s staffing levels in the future to increase oversight of its deputy ethics officials. Federal regulations and guidance specify that EPA has certain oversight responsibilities for its programs—including its ethics program. For example, OGE regulations: state that designated agency ethics officials, acting directly or through other officials, are responsible for carrying out effective financial disclosure programs by, among other things, using information in financial disclosure reports to prevent and resolve potential conflicts of interest; specify actions the official must take if the reviewing official concludes that information disclosed in the report may reveal a violation of applicable laws and regulations; and state that designated agency ethics officials are responsible for periodically evaluating their agencies’ ethics programs. Standards for Internal Control in the Federal Government also states that management should design control activities to achieve objectives and respond to risks, such as by comparing actual performance to planned or expected results and analyzing significant differences. Because EPA had not periodically evaluated through audits or spot- checks the quality of financial disclosure reviews for SGEs appointed to advisory committees, the agency was not well positioned to compare the program’s actual performance with planned results or address instances of noncompliance with federal ethics requirements. Until EPA’s Ethics Office, as part of its periodic review of its ethics program, evaluates—for example, through audits or spot-checks—the quality of financial disclosure reviews conducted for SGEs appointed to EPA advisory committees, it will not have reasonable assurance that it is addressing noncompliance with federal ethics requirements and preventing conflicts of interest among SGEs appointed to EPA advisory committees. EPA officials acknowledged that taking this additional oversight measure could enhance the agency’s ethics program. Of the four characteristics we reviewed—committee composition, regional affiliation, membership turnover, and number of committee meetings— one or more of the first three characteristics changed notably for four of 18 of EPA’s advisory committees after January 2017. There were no notable changes in the four characteristics we reviewed for the other 14 committees for which we reviewed at least one of the characteristics. The committee composition, regional affiliation, or membership turnover of four of EPA’s advisory committees changed notably after January 2017 compared to the period after January 2009. There was no notable change in the fourth characteristic we reviewed—that is, the number of meetings committees held. Each change identified as notable had at least a 20 percentage point difference in the change to the characteristic after January 2017 compared to the period after January 2009. See appendix I for additional information about our methodology. There was a notable decrease in the percentage of members affiliated with academic institutions on the SAB and EPA Board of Scientific Counselors (BOSC) committees after January 2017 compared to the period after January 2009. Our analysis shows that the percentage of committee members with an academic affiliation serving on the SAB decreased by 27 percentage points, or from 77 percent (36 of 47 members) on January 19, 2017, to 50 percent (22 of 44 members) about 15 months later on March 31, 2018. There was little change in the period after January 2009, when the percentage of academic members serving on the SAB remained stable at 83 percent (33 of 40 members) on January 19, 2009, and 82 percent (32 of 39 members) about 15 months later on March 31, 2010. Regarding 2013, academic members serving on the SAB decreased from 82 percent (40 of 49 members) on January 20, 2013 to 73 percent (37 of 51 members) about 15 months later. In addition to academic members, other members serving on the SAB are (1) affiliated with government (federal, local, state, or tribal) or with industry or non-government organizations (NGO); (2) are consultants; or (3) are others we could not assign to one of the above categories. See figure 3. BOSC also experienced a notable decrease in the percentage of members with an academic affiliation serving on the committee after January 2017 compared to the period after January 2009. Our analysis shows that the percentage of committee members with an academic affiliation serving on BOSC decreased by 45 percentage points, or from 65 percent (11 of 17 members) on January 19, 2017, to 20 percent (3 of 15 members) about 15 months later on March 31, 2018. There was little change in the percentage of academic members serving on BOSC after either January 2009 or January 2013. The percentage of members with an academic affiliation serving on BOSC was 55 percent (6 of 11 members) on January 19, 2009, and 56 percent (5 of 9 members) about 15 months later on March 31, 2010. Seven of 12 members were affiliated with academic institutions on January 20, 2013, and 5 of 9 members were similarly affiliated about 15 months later. See table 2. The regional affiliation of SAB committee members also changed notably after January 2017 compared to the period after January 2009. Our analysis shows that members affiliated with the southern region—which spans from Texas to Delaware—increased by about 25 percentage points, or from 28 percent (13 of 47 members) on January 19, 2017, to 52 percent (23 of 44 members) about 15 months later on March 31, 2018. There was little change in the period after January 2009, when the percentage of members affiliated with the southern region increased from 30 percent (12 of 40 members) on January 19, 2009, to 33 percent (13 of 39 members) about 15 months later on March 31, 2009. Regarding 2013, members affiliated with the southern region decreased from 33 percent (16 of 49 members) on January 20, 2013, to 27 percent (14 of 51 members) about 15 months later. Figure 4 shows the regional affiliation of SAB members using U.S. Census regions after January 2017 and January 2009. There was also a notable change in the number of members who left three committees after January 2017 compared to the number of members who left those committees after January 2009. Our analysis shows that of the members serving on January 19, 2017, 71 percent (12 of 17 members) of BOSC, 62 percent (23 of 37 members) of the Clean Air Act Advisory Committee, and 63 percent (25 of 40 members) of the Pesticide Program Dialogue Committee were no longer serving about 15 months later on March 31, 2018. There was little change in the period after January 2009, when 18 percent (2 of 11 members) of the members of BOSC and 3 percent (one of 35 members) of the members serving on the Clean Air Act Advisory Committee on January 19, 2009, were no longer serving on the committees about 15 months later on March 31, 2010. All of the members serving on the Pesticide Program Dialogue Committee (34 members) on January 19, 2009, were also serving about 15 months later on March 31, 2010. Regarding 2013, 25 percent (3 of 12 members) serving on BOSC on January 20, 2013, were not serving about 15 months later. All members serving on the other two committees on January 20, 2013, were also serving about 15 months later. In most instances, the four characteristics that we analyzed—committee composition, regional affiliation, membership turnover, and number of committee meetings held—did not change notably for the committees we reviewed from January 2017 to about 15 months later compared to the same time frame after January 2009. In many of these instances, the characteristics we analyzed had changed, but these changes were not large enough to be considered notable based on the approach we used to identify notable changes. Other than the SAB and BOSC, there were no notable changes after January 2017 in the composition of the five committees for which we analyzed this characteristic. We analyzed the committee composition of the three other committees combined because they did not have enough members to make individual analysis meaningful. Our analysis shows that the largest change after January 2017 that we did not identify as notable also occurred with BOSC. The percentage of members serving on BOSC with a government affiliation increased by 22 percentage points, or from 18 percent (3 of 17 members) on January 19, 2017, to 40 percent (6 of 15 members) about 15 months later on March 31, 2018. This compares to 2009 when the percentage of members serving on BOSC with a government affiliation remained at zero percent on January 19, 2009, (11 members) and about 15 months later on March 31, 2010, (9 members). Other than the SAB, there were no notable changes after January 2017 in the regional affiliation of members of the 10 committees for which we analyzed this characteristic. In addition to the SAB, we analyzed the regional affiliation of three other committees individually and the remaining six committees combined. The largest change in regional affiliation after January 2017 that we did not identify as notable also occurred with the SAB. Members affiliated with the northeast region decreased by more than 14 percentage points, or from 28 percent (13 of 47 members) on January 19, 2017, to 14 percent (6 of 44 members) about 15 months later on March 31, 2018. This compares to 2009 when the percentage of members affiliated with the northeast region stayed about the same, changing from 20 percent (8 of 40 members) on January 19, 2009, to 18 percent (7 of 39 members) about 15 months later on March 31, 2010. Other than BOSC, the Clean Air Act Advisory Committee, and the Pesticide Program Dialogue Committee, there were no notable changes after January 2017 to membership turnover for the 14 committees for which we analyzed this characteristic. In addition to these three committees, we analyzed the membership turnover of six other committees individually and the remaining five committees combined. Our analysis shows that the largest change in membership turnover after January 2017 that we did not identify as notable occurred with the SAB. Of the members serving on this committee on January 19, 2017, 45 percent (21 of 47 members) were no longer serving about 15 months later on March 31, 2018. This compares to 2009 when 35 percent (14 of 40 members) serving on January 19, 2009, were not serving about 15 months later on March 31, 2010. There was no notable change in the percentage decrease of meetings held before and after January 2017 compared to a similar time frame before and after January 2009. We analyzed the number of meetings held by 18 committees. Our analysis shows that for the 18 committees combined, the number of meetings decreased by 40 percent (from 90 to 54 meetings) from the approximately 15 month period before January 2017 to the approximately 15 month period after January 2017. This compares to a 27 percent decrease in meetings (from 164 to 120 meetings) from the approximately 15-month period before January 2009 to the approximately 15-month period after January 2009. Overall, there was a decrease in the number of meetings from before January 2009 to after January 2017. The number of meetings held by the 18 committees combined decreased 67 percent (from 164 to 54 meetings) from the approximately 15-month period before January 2009 to the approximately 15-month period after January 2017. Figure 5 illustrates the decrease in the number of meetings held during this time frame. The figure shows the number of meetings held by SAB separately because of the relatively large number of meetings that it held relative to the other committees. EPA’s federal advisory committees play an important role in advising the agency. EPA generally followed its established process for 15 of the 17 advisory committees that appointed or reappointed committee members during the time period we reviewed. However, EPA did not follow a key step in its process for appointing 20 members to two committees that advise the agency on environmental regulatory matters, among other things. The agency did not prepare draft membership grids with staff rationales for proposed membership, the documents intended to reflect EPA staff input on the best qualified and most appropriate candidates for achieving balanced committee membership before appointing these members. EPA officials told us in March 2019 that they did not prepare draft membership grids, as recommended by EPA’s Federal Advisory Committee Handbook, because EPA management requested a series of briefings instead. There may be benefits to following different procedures; however, under EPA’s established process, agency staff are to document in draft membership grids and include in appointment packets their rationales for recommending the candidates they deem best qualified and most appropriate for achieving balanced committees. By directing officials responsible for appointing committee members to prepare draft membership grids and include them in appointment packets for all committees, the agency would have better assurance that it could show how it made appointment decisions to achieve the best qualified and most appropriate candidates for balanced committee membership. EPA also did not consistently ensure that committee members appointed as SGEs met federal ethics requirements, and as part of its periodic review of its ethics program, EPA did not evaluate through audits or spot- checks the quality of financial disclosure reviews conducted by deputy ethics officials for these committee members. Until EPA’s Ethics Office periodically evaluates—for example, through audits or spot-checks—the quality of financial disclosure reviews conducted for SGEs appointed to EPA advisory committees, it will not have reasonable assurance that it will address noncompliance with federal ethics requirements and prevent conflicts of interest among SGEs appointed to EPA advisory committees. We are making the following two recommendations to EPA: The EPA Administrator should direct EPA officials responsible for appointing advisory committee members to follow a key step in its appointment process—developing and including draft membership grids in appointment packets with staff rationales for proposed membership— for all committees. (Recommendation 1) EPA’s Designated Agency Ethics Official should direct EPA’s Ethics Office, as part of its periodic review of EPA’s ethics program, to evaluate—for example, through audits or spot-checks—the quality of financial disclosure reviews for special government employees appointed to EPA advisory committees. (Recommendation 2) We provided a draft of this report to EPA for review and comment. In its written comments, reproduced in appendix IV, EPA disagreed with a key finding related to the first recommendation, with how we conducted some of our data analyses, and with some of the data points we presented. EPA agreed with the findings and conclusions related to the second recommendation. EPA also provided other comments, which we incorporated as appropriate. EPA stated that it believed a key finding related to the draft report’s first recommendation—that EPA follow, for all committees, the key step in its appointment process related to developing draft membership grids—was in error and should be removed from the final version of the report. EPA also stated that it followed all membership steps outlined in agency guidance with the exception of two committees, SAB and CASAC, who substituted the development of a membership grid with what the agency states was a more rigorous examination of the candidates (a series of briefings with senior management discussing the strengths and weaknesses of potential candidates). EPA stated that this is within the discretion of the EPA Administrator and that the vetting of candidates for SAB and CASAC occurred in a different manner than in previous years with a process more robust than membership grids. In addition, EPA stated that the public comment process was more robust, going beyond what was prescribed in the traditional membership process. According to EPA, for SAB and CASAC, the public was offered additional opportunity to provide input on all nominated candidates under consideration. We agree that conducting such briefings is within the discretion of the EPA Administrator, and we did not assess the outcomes of the membership appointment process. However, it remains that for SAB and CASAC, EPA did not follow a key step in its established appointment process—as documented in its agency-wide handbook—in which agency staff are to document in draft membership grids their rationales for recommending the candidates they deem best qualified and most appropriate for achieving balanced committees. While there may be benefits to following any number of alternative processes for appointing committee members, as EPA stated in its Federal Advisory Committee Advisory Handbook, EPA developed the handbook to help agency officials comply with FACA requirements. For these two advisory committees, EPA did not follow its established committee appointment process, impeding EPA’s ability to ensure that it consistently meets— across all of its advisory committees—FACA’s purpose of encouraging uniform committee appointment procedures. Furthermore, EPA did not provide documentation of the “more rigorous examination” of candidates it conducted in briefings. In its written comments, EPA stated that the SAB Staff Office documented staff evaluations in briefing documents and that we did not request such documents. However, we requested all appointment packets for the 17 committees that appointed or reappointed committee members from fiscal year 2017 through the first two quarters of fiscal year 2018. These appointment packets were to contain the documents used by EPA management to make appointment and reappointment decisions. EPA did not include the briefing documents in their packets for the SAB or CASAC, impeding EPA’s ability to ensure that it consistently meets— across all of its advisory committees—FACA’s purpose of encouraging uniform committee appointment procedures. Nor did the agency provide any such documentation in subsequent discussions about the extent to which the agency followed its established process. Our most recent meeting with EPA took place on March 19, 2019. As appropriate, we modified the report to further clarify our specific finding. Moreover, EPA disagreed with how we conducted some of our data analyses and with some of the data points we presented. We took numerous steps to ensure the accuracy of the data points presented in this report. In some instances, we identified missing or inconsistent data and shared this information with EPA officials. EPA provided some corrected data for members with missing or inconsistent appointment- date data from October 1, 2015 to March 31, 2018. We also asked EPA staff to confirm that the data had been updated in the FACA database, discussing the data with individual EPA staff members, conducting logic tests and spot-checking the data to identify errors and inconsistencies, and providing EPA with an opportunity to review and correct in writing the data presented prior to preparing our draft report. Also, in its written comments, EPA stated that we did not review data for BOSC subcommittees. Our methodology focused on the composition of committees and not their subcommittees. We continue to believe that the methodology we employed to analyze data was appropriate. We outline our rationale in appendix I, which includes the steps we took to ensure data reliability. For these reasons, we do not plan to make any further changes based on the additional data EPA provided. Lastly, EPA did not dispute our findings and conclusions related to the second recommendation that the agency evaluate, for example, through audits or spot checks, the quality of financial disclosure reviews for special government employees appointed to EPA advisory committees. EPA noted that at the time of our audit, its Ethics Office was understaffed. In its written comments, EPA said that it has now resolved these staffing issues and is engaged in a full and thorough review of all employees’ (including special government employees serving on federal advisory committees) ethics forms to ensure they meet all ethics requirements. As agreed with your offices, unless you publicly announce the contents of this report earlier, we plan no further distribution until 30 days from the report date. At that time, we will send copies of this report to the appropriate congressional committees, the Administrator of the U.S. Environmental Protection Agency, the Administrator of the U.S. General Services Administration, and the Director of the U.S. Office of Government Ethics. In addition, the report will be available at no charge on the GAO website at http://www.gao.gov. If you or your staff members have any questions about this report, please contact me at (202) 512-3841 or gomezj@gao.gov. Contact points for our Offices of Congressional Relations and Public Affairs may be found on the last page of this report. GAO staff members who made major contributions to this report are listed in appendix V. To describe the U.S. Environmental Protection Agency’s (EPA) established process for appointing members to serve on EPA advisory committees, we identified and reviewed the federal laws, regulations, and policies that are relevant to EPA’s process for appointing advisory committee members. To ensure that we correctly identified all relevant laws, regulations, and guidance, we consulted with: (1) the Committee Management Secretariat at the U.S. General Services Administration (GSA), which issues regulations and guidance for Federal Advisory Committee Act (FACA) committees government-wide; (2) the U.S. Office of Government Ethics, which develops ethics-related regulations for executive branch employees; and (3) EPA. Examples of EPA guidance that we reviewed include EPA’s Federal Advisory Committee Handbook, Strengthening and Improving Membership on EPA Federal Advisory Committees, and EPA Ethics Advisory 2008-02. To evaluate the extent to which EPA followed its established process for appointing members from fiscal year 2017 through the first two quarters of fiscal year 2018, we reviewed pertinent documentation from the 17 committees that appointed or reappointed advisory committee members during this time frame. The remaining committees did not appoint any committee members during the time frame we reviewed. For the above- mentioned 17 committees, we reviewed all advisory committee appointment packets—each of which can contain appointment documents for numerous appointees or reappointees—produced during this time. We also reviewed the first section (Section 1: Identifying Information and Record of Agency Review) of the Confidential Financial Disclosure Form for EPA Special Government Employees (EPA Form 3110-48) for 74 individuals who were required to submit them to EPA to determine if they met federal financial-disclosure-reporting requirements. We reviewed all 74 of the forms provided by the 8 committees that appointed or reappointed special government employees (SGE) to serve on a committee from fiscal year 2017 through the first two quarters of fiscal year 2018. Additionally, we interviewed EPA officials involved with appointing committee members to understand the steps these officials took. We then compared the steps they described taking with selected steps in EPA’s established process for appointing members to evaluate the extent to which the agency followed its process. We focused on steps in the appointment process that were to be documented in the appointment packets, which EPA used to support appointment decisions. Specifically, we reviewed those aspects of the process for which EPA had documentary evidence, and we evaluated the implementation of ethics oversight requirements that are relevant to EPA’s committee-member appointment process. To determine whether the agency followed selected steps in its established process, two senior analysts reviewed the appointment packets. Specifically, one senior analyst conducted the primary analysis for about half of the 22 appointment packets we received, while the other conducted the primary analysis for the remaining packets. Afterwards, each analyst reviewed the other’s conclusions and noted agreement or disagreement based on the evidence provided. In some cases, discussion was necessary to resolve differences of opinion between the two analysts. Those discussions were documented. If additional documentation was necessary to resolve differences of opinion, we obtained additional information from the agency. The two analysts reached agreement on all of the packets. To describe how, if at all, selected characteristics of EPA’s advisory committees changed after January 2017, we analyzed information from the FACA database, a publically-available database maintained by GSA. The database contains information about FACA advisory committees that agencies, including EPA, are required to provide. The initial scope of our review was the 22 committees in existence on March 31, 2018. Of these 22 committees, we excluded from all of our analyses the four committees that were established after November 2007 because this is the earliest date of one of our analyses. We also excluded four other committees from the three analyses that rely on member appointment start and end dates (committee composition, membership turnover, and regional affiliation) because of missing or inconsistent data. Additionally, we excluded some other committees from some of our analyses because of other types of data reliability issues or because of the nature of the characteristic. To assess the reliability of the committee data, we reviewed database technical documentation and interviewed GSA and EPA officials to identify any potential issues with our planned analysis of the data, among other things, and determined that overall the data were sufficiently reliable for conducting analysis to describe changes in selected member and committee characteristics for our selected time periods. We discuss additional steps we took to assess the reliability of the data and data reliability issues with the FACA database at the end of this appendix. Additionally, appendix II identifies which committees we excluded from which analyses and the reasons why. Primarily using information available in the FACA database, we compared changes in four committee characteristics across committees and changes in presidential administrations. Specifically, we measured the characteristics before and after January 20, 2017, and compared them to similar periods before and after January 20, 2009. Additionally, we also compared the characteristics to those before and after January 21, 2013, to provide context to our findings and identify any patterns over time in the data. The four characteristics we measured and compared across committees and changes in presidential administrations were: Number of committee meetings For the first two characteristics, we compared across committees the percentage of members in the characteristics’ categories on either January 19, 2017, or January 19, 2009, to a day about 15 months later (either March 31, 2010, or March 31, 2018). For membership turnover, we compared across committees the percentage of members on either January 19, 2017, or January 19, 2009, who left a committee by about 15 months later (either March 31, 2010, or March 31, 2018). We chose March 31, 2018, to allow for a period of time after January 2017 for changes to occur in committee characteristics, and the fiscal year 2018 data file we received from GSA was updated as of March 31, 2018. For the fourth characteristic, we compared across committees the number of meetings held in the 15 months before January 20, 2009 and January 20, 2017, to a similar period after those dates (November 12, 2007, to March 31, 2010, or November 12, 2015, to March 31, 2018). To identify changes to a characteristic that were notable, we used the following methodology. First we identified any changes after January 2017 that were large relative to other changes to that characteristic after January 2017. If we identified a relatively large change, we then compared it to changes to the characteristic after January 2009 to assess whether it was large relative to those changes. If it was, we would identify the change as notable. The committees we analyzed individually had at least 10 members (or 10 meetings) in the relevant time periods being measured, with the exception of two committees which had nine members on March 31, 2010. We analyzed the other committees combined since relatively small changes in counts would have a relatively large impact on percentages. We measured the committee composition of 5 of 18 committees. We excluded 4 of the 18 committees because of data reliability issues and 9 committees because they were not staffed primarily with SGEs. We limited the committee composition analysis to SGEs because SGEs are expected to provide their best judgement free from conflicts of interest, rather than represent a particular viewpoint. We analyzed two of the five committees individually and the other three committees combined. To measure the composition of the five committees, we first categorized each member’s occupation from the “occupation/affiliation” field in the FACA database into one of six categories. The categories were: non-government organization (NGO); or other. To assign the categories, one GAO analyst reviewed the occupation/affiliation data for each member and assigned one of five categories (academic, consultant, government, industry, or NGO) to each member. In instances where it was unclear what category to assign, the analyst conducted online searches regarding the occupation/affiliation information to identify the type of entity and assign a category. We assigned the category “other” in 30 instances where the member was affiliated with more than one of the other categories, not affiliated with any of the other categories (for example, retired), or for which the FACA database did not provide sufficient information to assign one of the other categories. A second analyst reviewed the reasonableness of the categories assigned by the first analyst—including the additional research. The two analysts reached consensus on the categories for each member. We then applied the methodology described above to identify notable changes in committee composition after January 2017. We measured the regional affiliation of 10 of 18 committees. We excluded 8 committees because of data reliability issues. We analyzed 4 of the 10 committees individually and the other 6 committees combined. To measure the regional affiliation of the 10 committees, we assigned one of four U.S. Census regions (as defined by the U.S. Census Bureau) to each committee member based on data in the “occupation/affiliation” field in the FACA database for that member—in most instances, state information is included in this field. We then applied the methodology described above to identify notable changes in regional affiliation to the period after January 2017. The regions were: Western. We measured membership turnover in 14 of 18 committees. We excluded 4 committees because of data reliability issues. We analyzed 9 of the committees individually and the other 5 committees combined. To measure membership turnover of the 14 committees, we used date fields indicating when committee members began and ended their terms to determine the percentages of members on a committee on January 19, 2017, and January 19, 2009, who were not members about 15 months later. We then applied the methodology described above to identify notable changes in membership turnover after January 2017. We measured the change in the number of meetings for 18 committees. We analyzed two of the committees individually and the other 16 committees combined. To measure this characteristic, we used data on the date that meetings were held (we used the date that the meeting began if it was a multi-day meeting). We then applied the methodology described above to identify notable changes in the number of meetings after January 2017. We assessed the reliability of the data provided to us by GSA and took certain steps to prepare the data for analysis. GSA provided us with data files downloaded to Excel from its FACA database from October 1, 2005, to March 31, 2018, for our analysis. GSA maintains the FACA database on a fiscal year basis. During the fiscal year, staff in each agency, including EPA, are to enter data to reflect any changes about the agency’s FACA committees. At the end of each fiscal year, GSA is to perform, in conjunction with each agency, an annual comprehensive review of the data entered into the database by the agency for that fiscal year. According to GSA officials, these reviews constitute the agency’s main process for ensuring the reliability of the database. Once the review is complete, the data are locked down, meaning they can no longer be changed. We received data through the 2017 fiscal year after GSA completed the 2017 review. Because this latest GSA review was the end of fiscal year 2017 and we wanted to include data into 2018, we requested that EPA update the database to March 31, 2018, for each committee for certain data fields relevant to our analyses. We asked that for each committee, the EPA staff member responsible for entering a committee’s data in the FACA database provide confirmation to us that the data had been updated through March 31, 2018. After we received confirmation that data for the 22 committees in existence on March 31, 2018, had been updated, GSA staff provided us the data update for EPA committees from October 1, 2017, through March 31, 2018. To further assess the reliability of these data, we reviewed the database’s technical documentation and interviewed GSA and EPA officials to identify any potential issues with our planned analysis of the data. We conducted logic tests and spot-checked the data to identify errors and inconsistences. For example, we scanned committee member’s names to identify potential duplicates of the same person in the same committee and made corrections where appropriate. If a person served on more than one committee, we included that person separately for each committee on which he or she served. For each member, we also checked the appointment start and end dates indicated in each fiscal year for inconsistencies across fiscal years. In some instances, we identified missing or inconsistent data in these dates and shared this information with EPA officials. EPA was able to provide some corrected data for members with missing or inconsistent appointment-date data from October 1, 2015, to March 31, 2018. We excluded from our analyses four committees for which over 30 percent of members had appointment date issues we were not able to resolve, as well as individual members with unresolved date issues for the committees we included in the analysis. We also checked the 2018 data that GSA provided to us against the data posted to EPA’s website. We determined that overall the data were sufficiently reliable for conducting analysis to describe changes in selected member and committee characteristics for our selected time periods. Finally, we took steps to structure the data provided by GSA in the format needed for our analyses. Specifically, because GSA maintains its data on a fiscal year basis, the data we received from GSA contained a separate row in the database for each committee member for each fiscal year that he or she was a member. To facilitate our analyses, we transposed the dataset so there was one row for each member (for each committee, if a member was in more than one committee) that contained the data from all of the fiscal year records for that member. We conducted this performance audit from October 2017 to July 2019 in accordance with generally accepted government auditing standards. Those standards require that we plan and perform the audit to obtain sufficient, appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives. We believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives. Table 3 provides information about each of the 22 advisory committees managed by the U.S. Environmental Protection Agency (EPA) as of March 31, 2018. For each of these committees, the table also identifies whether we included it in one or more of our analyses. If we excluded a committee from certain analyses, we also explain why. Table 4 summarizes the number of advisory-committee appointment packets for which the U.S. Environmental Protection Agency (EPA) did or did not follow the steps we evaluated for appointing members to serve on EPA advisory committees. In addition to the individuals named above, Joseph Thompson (Assistant Director), John Delicath, Charles Egan, Chad Gorman, Richard Johnson, Yvonne Jones, Mary Koenen, James Lager, Amber Sinclair, and Kiki Theodoropoulos made important contributions to this report.\n",
      "\n",
      "Summary:\n",
      "Federal advisory committees provide advice to federal agencies on many topics. As of March 31, 2018, EPA managed 22 such committees. They advise the agency on such issues as developing regulations and managing research programs. Questions have been raised about EPA's process for appointing committee members after recent policy changes affecting who serves on the advisory committees. GAO was asked to review issues related to how EPA appoints advisory committee members. This report examines: (1) EPA's process for appointing advisory committee members, (2) the extent to which EPA followed its process for selecting members from October 2016 through March 2018, and (3) how, if at all, selected characteristics of EPA advisory committees changed after January 2017. GAO reviewed relevant federal laws, regulations, and guidance; reviewed documents from committees that appointed members over this period; analyzed information from the GSA's FACA database; and interviewed agency officials. Based on GAO's review of U.S. Environmental Protection Agency's (EPA) guidance, the agency's established process for appointing advisory committee members involves three main phases: soliciting nominations, evaluating candidates, and obtaining approvals. Each phase involves several steps. For example, a key step for evaluating candidates involves EPA staff's preparing documents that reflect staff recommendations on the best qualified and most appropriate candidates for achieving balanced committee membership, according to EPA guidance. EPA generally followed its established process for most of its 22 advisory committees; however, in fiscal year 2018, EPA did not follow a key step for appointing 20 committee members to two committees GAO reviewed: the EPA Science Advisory Board and Clean Air Scientific Advisory Committee, which advise the agency on environmental regulatory matters, among other things. The 2018 appointment packets for these two committees did not contain documents reflecting EPA staff rationales for proposed membership, as called for by EPA's established process. EPA developed guidance to implement the Federal Advisory Committee Act (FACA). By directing officials responsible for appointing committee members to follow a key step in its process to document staff rationales for proposed membership, the agency would have better assurance that it will (1) consistently meet FACA's purpose of encouraging uniform appointment procedures and (2) show how it made appointment decisions to achieve the best qualified and most appropriate candidates for balanced committee membership. EPA also did not consistently ensure that members appointed as special government employees (SGE)—who are expected to provide their best judgment free from conflicts of interest and are required by federal regulations to disclose their financial interests—met federal ethics requirements. For about 23 percent, or 17 of the 74 financial disclosure forms GAO reviewed, an ethics official had not signed and dated that the SGE filing the form was in compliance with federal ethics rules. EPA also did not periodically review its ethics program, as called for by federal regulations, such as through audits or spot-checks, to evaluate the quality of financial disclosure reviews for SGEs. Until EPA's Ethics Office evaluates the quality of financial disclosure reviews of SGEs as part of its periodic review of its ethics program, it will not have reasonable assurance that it will address noncompliance with federal ethics requirements and prevent conflicts of interest on its advisory committees. Based on GAO's review of the U.S. General Services Administration's (GSA) FACA database, there were notable changes to selected characteristics of EPA advisory committees (i.e. at least a 20 percentage point difference in the change to a characteristic after January 2017 compared to the period after January 2009). Of the four characteristics GAO reviewed—committee composition, regional affiliation, membership turnover, and number of meetings committees held—one or more of the first three changed notably for four of 18 EPA advisory committees after January 2017. GAO is recommending that EPA direct (1) officials responsible for appointing committee members to follow a key step in its appointment process to document staff rationales for proposed membership and (2) EPA's Ethics Office to evaluate the quality of financial disclosure reviews of SGEs appointed to advisory committees. EPA disagreed with the first and agreed with the second recommendation. GAO continues to believe that both are valid, as discussed in the report.\n"
     ]
    }
   ],
   "source": [
    "# Load all the data\n",
    "datasetDict = load_dataset(\"ccdv/govreport-summarization\")\n",
    "# this dataset is a dict with train, validation, and test\n",
    "\n",
    "# get lengths of dataset splits\n",
    "split_lengths = [len(datasetDict[split])for split in datasetDict]\n",
    "\n",
    "# Print our dataset for sanity checking\n",
    "print(\"Our dataset:\")\n",
    "splits = [f\"Set '{split}': rows:{datasetDict[split].num_rows}, features:{datasetDict[split].column_names}\" for split in datasetDict]\n",
    "print(\"\\n\".join(splits))\n",
    "\n",
    "# Print a a sample report and summary\n",
    "print(\"\\nReport:\")\n",
    "\n",
    "print(datasetDict[\"test\"][1][\"report\"])\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "\n",
    "print(datasetDict[\"test\"][1][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam\\AppData\\Local\\Temp\\ipykernel_15108\\2274826437.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric('rouge')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Adam\\Desktop\\Projects\\flashGovProject\\flaskserver\\models\\modelTesting2.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/modelTesting2.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# testing the braking down of the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/modelTesting2.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m rouge_metric \u001b[39m=\u001b[39m load_metric(\u001b[39m'\u001b[39m\u001b[39mrouge\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/modelTesting2.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m score \u001b[39m=\u001b[39m calculate_metric_on_test_ds(datasetDict[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m], rouge_metric, model, tokenizer)\n",
      "\u001b[1;32mc:\\Users\\Adam\\Desktop\\Projects\\flashGovProject\\flaskserver\\models\\modelTesting2.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/modelTesting2.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_metric_on_test_ds\u001b[39m(dataset, metric, model, tokenizer, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/modelTesting2.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, device\u001b[39m=\u001b[39mdevice, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/modelTesting2.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                                column_text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreport\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/modelTesting2.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                                column_summary\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/modelTesting2.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/modelTesting2.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m#Get the batch of text and associated summary\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/modelTesting2.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     text_batches \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(generate_batch_sized_chunks(dataset[column_text], batch_size))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/modelTesting2.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     target_batches \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Desktop/Projects/flashGovProject/flaskserver/models/modelTesting2.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfinished text batches\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\datasets\\arrow_dataset.py:2778\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2776\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2777\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2778\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\datasets\\arrow_dataset.py:2763\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2761\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2762\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, key, indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 2763\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[0;32m   2764\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39;49mformatter, format_columns\u001b[39m=\u001b[39;49mformat_columns, output_all_columns\u001b[39m=\u001b[39;49moutput_all_columns\n\u001b[0;32m   2765\u001b[0m )\n\u001b[0;32m   2766\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\datasets\\formatting\\formatting.py:624\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    622\u001b[0m python_formatter \u001b[39m=\u001b[39m PythonFormatter(features\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m format_columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[39mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[39m=\u001b[39;49mquery_type)\n\u001b[0;32m    625\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    626\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\datasets\\formatting\\formatting.py:398\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_row(pa_table)\n\u001b[0;32m    397\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 398\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_column(pa_table)\n\u001b[0;32m    399\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    400\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_batch(pa_table)\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\datasets\\formatting\\formatting.py:437\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_column\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m    436\u001b[0m     column \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_arrow_extractor()\u001b[39m.\u001b[39mextract_column(pa_table)\n\u001b[1;32m--> 437\u001b[0m     column \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpython_features_decoder\u001b[39m.\u001b[39;49mdecode_column(column, pa_table\u001b[39m.\u001b[39;49mcolumn_names[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m    438\u001b[0m     \u001b[39mreturn\u001b[39;00m column\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\datasets\\formatting\\formatting.py:217\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_column\u001b[1;34m(self, column, column_name)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode_row\u001b[39m(\u001b[39mself\u001b[39m, row: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m    215\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures\u001b[39m.\u001b[39mdecode_example(row) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39melse\u001b[39;00m row\n\u001b[1;32m--> 217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode_column\u001b[39m(\u001b[39mself\u001b[39m, column: \u001b[39mlist\u001b[39m, column_name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures\u001b[39m.\u001b[39mdecode_column(column, column_name) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39melse\u001b[39;00m column\n\u001b[0;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode_batch\u001b[39m(\u001b[39mself\u001b[39m, batch: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# testing the breaking down of the model\n",
    "\n",
    "rouge_metric = load_metric('rouge')\n",
    "\n",
    "score = calculate_metric_on_test_ds(datasetDict[\"train\"], rouge_metric, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization', model = model_ckpt )\n",
    "\n",
    "pipe_out = pipe(datasetDict['test'][0]['report'] )\n",
    "\n",
    "print(pipe_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FlashGov_ML_Server",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
