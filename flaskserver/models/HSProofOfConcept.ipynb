{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chunking: Proof of Concept\n",
    "- In the second attempt, we tried dividing the sentences into chunks.\n",
    "- In this case, the chunking was actually pretty advanced, we chunked by sentences, and split the remaining sentences up only when we needed to. In theory this would provide an input that would give a more coherent summary.\n",
    "- We then summarized those summaries, and it worked.\n",
    "- This is an example of Hierarchical Summarization.\n",
    "- What we want to do, is grab an existing model setup, and implement chunking and hierarchical summarization in place of the regular model prediction function.\n",
    "- For this, we want to restart to make more moduar and reusable code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch cuda version: 12.1\n",
      "Cuda available. 1 device(s) detected.\n",
      "Current Device: Number:0 Name:NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#import all and setup cuda\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(\"torch cuda version:\", torch.version.cuda)\n",
    "# Check for cuda availability\n",
    "if(torch.cuda.is_available()):\n",
    "    deviceCount = torch.cuda.device_count()\n",
    "    currentNumber = torch.cuda.current_device()\n",
    "    deviceName = torch.cuda.get_device_name(currentNumber) \n",
    "    print(f\"Cuda available. {deviceCount} device(s) detected.\")\n",
    "    print(f\"Current Device: Number:{currentNumber} Name:{deviceName}\")\n",
    "else:\n",
    "    print(\"Cuda not available\")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input read. Length is 4700\n"
     ]
    }
   ],
   "source": [
    "# Get input\n",
    "\n",
    "inputString = \"\"\n",
    "filePath = \"../testData/test.txt\"\n",
    "with open(filePath, \"r\") as file:\n",
    "    inputString = file.read()\n",
    "\n",
    "speakers = [\"Sean Casten\"]\n",
    "speakerText = \"\"\n",
    "if len(speakers) >  1:\n",
    "    speakerText = \"speeches by \" + [s for s in speakers].join(\", \")\n",
    "else:\n",
    "    speakerText = \"a speech by \" + speakers[0]\n",
    "prompt = f\"Please summarize this U.S. Government transcript of {speakerText}. Feel free to ignore things that look like this: '[[page number]]'\"\n",
    "inputString = prompt + inputString\n",
    "\n",
    "print(f\"Input read. Length is {len(inputString)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Adam\\anaconda3\\envs\\FlashGov_ML_Server\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the model\n",
    "from transformers import AutoModelForSeq2SeqLM, pipeline, AutoTokenizer\n",
    "\n",
    "\n",
    "modelID = \"google/flan-t5-base\"\n",
    "\n",
    "# Instantiate a pretrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(modelID)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Instantiate a pretrained tokenizer, use same as model\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max input length: 512, Max Sentence length: 511, SpecialTokens: 1\n"
     ]
    }
   ],
   "source": [
    "# Checking the limits we're working with\n",
    "maxInput = tokenizer.model_max_length # max length of input\n",
    "maxSentence = tokenizer.max_len_single_sentence # max len of a single sentince\n",
    "specialTokens = tokenizer.num_special_tokens_to_add() # tokenizer will add 2 special tokens for input seq\n",
    "print(f\"Max input length: {maxInput}, Max Sentence length: {maxSentence}, SpecialTokens: {specialTokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences:51, Max Sentence Len:63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Adam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# we need nltk to tokenize large inputs\n",
    "import nltk\n",
    "# punkt seems nessecary\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = nltk.tokenize.sent_tokenize(inputString)\n",
    "sentenceLen = len(sentences)\n",
    "\n",
    "# check max token length of all sentences once tokenized\n",
    "maxSentenceLen = max([len(tokenizer.tokenize(sentence)) for sentence in sentences])\n",
    "\n",
    "print(f\"Num of sentences:{sentenceLen}, Max Sentence Len:{maxSentenceLen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1076 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[507, 501, 67]\n",
      "[508, 502, 68]\n",
      "1075\n",
      "1076\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# holds working chunk seperated by whitespace\n",
    "chunk = \"\"\n",
    "# holds all completed chunks\n",
    "chunks = []\n",
    "\n",
    "length = 0\n",
    "count = -1\n",
    "\n",
    "for sentence in sentences:\n",
    "    count += 1\n",
    "    combined_length = len(tokenizer.tokenize(sentence)) + length\n",
    "    \n",
    "    # If the combined length is within permissable length\n",
    "    if(combined_length < tokenizer.max_len_single_sentence):\n",
    "        \n",
    "        # and the sentince and length to our working chunk\n",
    "        chunk += sentence + \" \"\n",
    "        length = combined_length\n",
    "        \n",
    "        # if this is the last chunk, strip whitespace and save it in chunks\n",
    "        if count == len(sentences) -1:\n",
    "            chunks.append(chunk.strip())\n",
    "    \n",
    "    # If it breaches the maxmimum allowed chunks\n",
    "    else:\n",
    "        # save the chunk we have\n",
    "        chunks.append(chunk.strip())\n",
    "        \n",
    "        # reset the length and chunk\n",
    "        length = 0\n",
    "        chunk = \"\"\n",
    "        \n",
    "        # add the overflowing chunk and update length\n",
    "        chunk += sentence + \" \"\n",
    "        length = len(tokenizer.tokenize(sentence))\n",
    "        \n",
    "        \n",
    "# =============== Sanity Checks =============== #\n",
    "# How many chunks we have\n",
    "print(len(chunks))\n",
    "# How many tokens are in each chunk (excluding special tokens)\n",
    "print([len(tokenizer.tokenize(c)) for c in chunks])\n",
    "# Number of tokens in eeach chunk (including spacial tokens)\n",
    "print([len(tokenizer(c).input_ids) for c in chunks])\n",
    "# total number of tokens in all chunks\n",
    "print(sum([len(tokenizer.tokenize(c)) for c in chunks]))\n",
    "# this should be close to the total number of chunks\n",
    "# if it's not, thats because we removed extra whitespaces while stripping\n",
    "print(len(tokenizer.tokenize(inputString)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our inputs using the tokenizer on our chunks\n",
    "# Send them to the correct device while we're at it\n",
    "inputs = [tokenizer(chunk, return_tensors=\"pt\").to(device) for chunk in chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      "input: [<class 'transformers.tokenization_utils_base.BatchEncoding'>, <class 'transformers.tokenization_utils_base.BatchEncoding'>, <class 'transformers.tokenization_utils_base.BatchEncoding'>]\n",
      "The U.S. Government transcript of a speech by Sean Casten.\n",
      "input: [<class 'transformers.tokenization_utils_base.BatchEncoding'>, <class 'transformers.tokenization_utils_base.BatchEncoding'>, <class 'transformers.tokenization_utils_base.BatchEncoding'>]\n",
      "The House would have voted to raise the debt ceiling, and President Biden would sign it.\n",
      "input: [<class 'transformers.tokenization_utils_base.BatchEncoding'>, <class 'transformers.tokenization_utils_base.BatchEncoding'>, <class 'transformers.tokenization_utils_base.BatchEncoding'>]\n",
      "We can pass a clean debt limit bill.\n",
      "cs <class 'str'>\n",
      "input: <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "MAIN SUM:Congress has a responsibility to fix the debt ceiling.\n"
     ]
    }
   ],
   "source": [
    "# F\n",
    "# print(\"input:\")\n",
    "# print(inputString)\n",
    "print(\"output:\")\n",
    "chunkSummaries = []\n",
    "for input in inputs:\n",
    "    print(\"input:\", [type(el) for el in inputs])\n",
    "    output = model.generate(**input, max_length=120)\n",
    "    decoded = tokenizer.decode(*output, skip_special_tokens=True)\n",
    "    chunkSummaries.append(decoded)\n",
    "    print(decoded)\n",
    "\n",
    "conactSummaries = \"\".join(chunkSummaries)\n",
    "print(\"cs\", type(conactSummaries))\n",
    "tokenizedSummaries = tokenizer(conactSummaries, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "print(f\"input: {type(tokenizedSummaries)}\")\n",
    "output = model.generate(**tokenizedSummaries, max_length=120)\n",
    "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"MAIN SUM:{decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
